{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Embedding : An introduction to Word2Vec and Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welcome!**\n",
    "\n",
    "In this notebook we first see an introduction about the concept of Distributional Semantic Models (DSM) and Word-Embedding and as we go on we'll learn about the Word2Vec and Doc2Vec algorithms and see how can we implement them in an ML task.\n",
    "\n",
    "Please note that the main purpose of this notebook is to make familiar a beginner ML user with the mentioned concepts instead of focusing on the most efficient - or pythonic - way to write the code. There are several ways the codes you'll see here can be written in a better/more intelligent way but as a result, they may seem more complex to a non-expert Python coder. That's why I intentionally decided to not use Classes, decorators and tools like those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is word-embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime between 50,000 years and 2 million years ago, humans started to talk and what we know today as \"languages\" is the result of this long process. \n",
    "\n",
    "The reason for it is relatively simple: to be able to communicate about the world around them, which directly impacts their lives. So, they create \"representations\" (which are not limited to words) to transfer an idea/concept/meaning to another human. While in the beginning, these words/sounds/... were just independent symbols, by the evolution of human language, words were also evolved not only as symbols but also as a way to link other concepts together. This process has happened for thousands of years. During this period, the human brain was unable to create a model of reality, to link words(symbols) not just assign them to a single concept/idea but to understand their dynamics with the other symbols. So when I ask you \"give me a pair of scissors to cut this rope \", if you can't find scissors and instead see a knife, you will probably bring it to me because in the model your brain has from reality, scissors are used to cut, just like a knife. While at first glance it may seem trivial, actually trying to replicate this phenomenon outside the human brain won't be so simple! Before going in deep to see what options are out, let us review how the human brain deals with words:\n",
    "\n",
    "<img src=\"Images/human_process.png\" height=\"300\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better grasp the concept of *creating representations*, let us take a look at a simple example:\n",
    "\n",
    "Imagine we have a bunch of apples, and for some reason, we want to find out the apples which are similar and dissimilar to each other:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/mele.png\" height=\"300\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How, in your opinion, we can represent an apple?\n",
    "\n",
    "Let us say we decide to *represent* each one using these characteristics:\n",
    "* Color\n",
    "* Height\n",
    "* Perimeter\n",
    "\n",
    "<img src=\"Images/apple_table.png\" height=\"300\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it is possible to show them in space; this means we just created a numerical representation for our apples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/coor.jpeg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did above is to get some _real-world_ objects and make a representation for them. Doing so, we area able to find the _similar_ apples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/apple_cluster.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, imagine that we have to do the same thing, but this time not for the entities with the physical feature but for words...not so easy, right? ;)\n",
    "We are lucky that we should not start from the first step, mapping concepts/ideas to symbols. Human beings have done it for us for thousands of years!\n",
    "Unfortunately, having words is not enough for creating our model. To do so, we need a numerical representation of textual data. Why do we need such a representation, you ask? Well, to find out, try to give the following paragraph to a classification algorithm, as it is, a text!\n",
    "\n",
    "``Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Discussion__\n",
    "\n",
    "How can we make a numerical representation for a text? Consider the following methods for creating numeric representation and try to discuss their disadvantages:\n",
    "\n",
    "- Word frequency\n",
    "- Word length\n",
    "- One-Hot coding\n",
    "- Word Similarity\n",
    "- ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be hard to believe, but the first scientific method regarding this question goes back to the 50s! Traditionally, researchers used _count or frequency methods_ to capture the context pattern of words. Later, these methods led to another group of methods: _prediction Based methods_ which apply dimensional reduction algorithms to a text corpus.\n",
    "In 2013, _Mikolov_ and hist team in _Google_ proposed a new way to generate the distributional Semantic model: __word2vec__ algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is Word2Vec the only way to perform a word-embedding? of course not! as we said above, there are decades of researchers working in this field, and you can find dozens of other methods in the literature. These methods can be grouped into two categories:\n",
    "\n",
    "* __Frequency Based Methods__\n",
    "    * Count Vectors\n",
    "    * TF-IDF\n",
    "    * Co-Occurance Vectors\n",
    "    * ...\n",
    "* __Predication Based Methods__\n",
    "    * Word2Vec\n",
    "    * GloVe\n",
    "    * Fasttext\n",
    "    * ...\n",
    "\n",
    "_Prediction Bases Methods are both more sophisticated and computationally expensive with respect to Frequency Based Methods._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the expected outcome of a Word2Vec model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, a Vector!\n",
    "\n",
    "To be more precise, a dense numerical vector with a pre-defined length.\n",
    "\n",
    "<img src=\"Images/word2vec.png\" height=\"300\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is just a simple neural network that performs a binary classification and uses the learned classifier weights as embeddings of words. All the classifier does is to return the probability of ___c___ being a context word for a target word ___t___. That's it!\n",
    "\n",
    "A Word2Vec model internally uses one of the following algorithms to create the embeddings:\n",
    "\n",
    "* Continuous Bag Of Words (CBOW)\n",
    "* Skip-Gram (SG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General mechanism of Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its immense power, behind the scenes Word2Vec algorithm has a simple concept. Let us use an example to understand better the internal mechanism.\n",
    "Before jumping to the example, we should emphasize one more time that the primary assumption behind the following parts is the theory of meaning or, as J.R.Firth puts it, _\"You shall know a word by the company it keeps\"_.\n",
    "How can we achieve such a goal, that is to understand the complex relationship between words and their context.\n",
    "\n",
    "Let's look at a simple example: Consider the left image in the following figure. Here we have a short text which contains words $A$ and $B$ (grey color). Now suppose that we find all the words that come immediately before and after our target words($A$ and $B$). We can see that these neighbours are almost the same (they are different combinations of $X$ and $Y$ ). In this case based on the meaning theory which we mentioned earlier, we can conclude that $A$ and $B$ have a similar concept given that their contexts ($X$ and $Y$) are similar. On the contrary, analyzing context words of $A$ and $B$ in the image on the right, we notice that the contexts of $A$ and $B$ are not similar( at least not as they were in the previous case), so we can say that in this case $A$ and $B$ do not have a similar concepts/meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/diff.png\" height=\"700\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just seen a highly simplified version of the Word2Vec algorithm. Let us see what the main differences between our example and the actual algorithm are:\n",
    "\n",
    "- **Length of text**: instead of dealing with a short document as in the example, Word2Vec usually performs on the large corpus of data.\n",
    "- **context**: unlike what we saw in the example, the actual example does not consider two words immediately before and after the target word. Instead, it considers a fixed window with a predefined length of preceding and following words concerning the target word.\n",
    "- **Scope**: While in our simple example, our goal was to check the similarity between two specific words, implementing the Word2Vec algorithm, we create a vector representation for all tokens present in the corpus. These vectors later allow us to perform further analysis like similarity and transformation, as we will see later in detail.\n",
    "- **Algorithm**: In the example, we compared the context of target words and based on the combination of context words, we have decided that $A$ and $B$ are similar in the image on the left and not similar in the image on the right. Word2Vec, on the other hand, uses a different approach that predicts the context from the target and vice versa. By doing so, it can understand which words have a similar context. (*CBOW* and *Skip-Gram algorithms*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen a simplified example and its main differences with Word2Vec let us see in more detail how Word2Vec works:\n",
    "\n",
    "To begin with, We need to have a large amount of textual data. It is important to notice that these data are not a collection of individual words but a big set of sentences (corpus). Assuming that our corpus has $T$ words, we can address each word like this:\n",
    "\n",
    "$$[ t = 1,2,...,T]$$\n",
    "\n",
    "Next, we assume that each word in this limited vocabulary is representable by a vector, and to start, we assign each word a random vector. Iterating through all corpus, we pick each word (centre word $w_{j}$) and assume that it (its representation in this case) should be able to predict its context. Each word's context is the words in a window with a length of $m$ around it. We repeat these iterations hundreds of thousands of times, changing the representation so that the prediction will be better. We can summarise what we have said until now in the following formula, which calculates the likelihood of our model:\n",
    "$$L(\\theta ) = \\prod_{t=1}^{T} \\prod_{-m\\leqslant j\\leqslant m} P(w_{t+j}\\mid w_{t};\\theta)$$\n",
    "$$j\\neq 0$$\n",
    "\n",
    "$\\boldsymbol{L}$: Model Likelihood or how good is our model in predicting context from centre words\n",
    "\n",
    "$\\boldsymbol{\\theta}$: Model variables we need to optimise. In our case, the only variable we have in hand is the vector representations of words. It means the only thing that defines our probability model's performance is the vectors we come with for each word in the corpus.\n",
    "\n",
    "\n",
    "$\\boldsymbol{m}$: The window size for the context words around the centre word\n",
    "\n",
    "$\\boldsymbol{P(w_{t+j}\\mid w_{t};\\theta)}$: Probability of having $w_{t+j}$ as a context word given the $w_{t}$ as center word and $\\theta$ as vector representation.\n",
    "\n",
    "While calculating the model performance (model likelihood) for a specific $\\theta$ is good, we need something that helps us find the best $\\theta$. We use a specific function called \"objective function\"(or loss function), which is a transformation of the model likelihood. Doing such a transformation, we will have $J(\\theta)$ which we can look at it as *Error* we get, assigning vectors to words. We are getting the best vectors possible for each word by minimising it. Let us take a look at this transformation and examine the internal pieces: \n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{T}\\log L(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-m\\leqslant j\\leqslant m}\\log P(w_{t+j}\\mid w_{t};\\theta)\n",
    "$$\n",
    "$$j\\neq 0$$\n",
    "\n",
    "As we said before $J(\\theta)$ is the transformed form of $L(\\theta)$. Let us see what the differences are:\n",
    "\n",
    "- Negative sign(-): indicates that we want to minimise the objective function. (we can achieve the same result also with maximising the objective function)\n",
    "- $\\frac{1}{T}$: By adding it, we calculate the Likelihood for every word in a corpus of $T$ words.\n",
    "- log: using logarithm (a usual operator in optimisation processes), we convert $\\prod$ to $\\sum$\n",
    "\n",
    "Before talking about the last piece, $P(w_{t+j}\\mid w_{t};\\theta)$, let us summarise what we said until this point: Our final goal is to find vectors that can understand the meaning behind words and can discover the similarities between them. In order to find such vectors, we have introduced a loss function which, by minimising it, we achieve our primary goal. Minimising this cost function means maximising the total probability that our model can predict the correct context from each word.\n",
    "\n",
    "Let us see a simple example: Imagine that we are dealing with a corpus with only five words and let us consider the window size as 1. so:\n",
    "$$T=5\\,\\,\\,and\\,\\,\\,m=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/w2v_schema.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can expand the loss function as:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "L(\\theta_{i}) & = -\\frac{1}{5} \\sum_{t=1}^{5}\\sum_{-1\\leqslant j\\leqslant 1}\\log P(w_{t+j}\\mid w_{t}) \\\\\n",
    " & = -\\frac{1}{5}(\\log\\,P(w_2\\mid w_1)+ \\\\\n",
    " & \\log P(w_1\\mid w_2) + \\log P(w_3\\mid w_2)+\\\\\n",
    " & \\log P(w_2\\mid w_3) + \\log P(w_4\\mid w_3)+\\\\\n",
    " & \\log P(w_3\\mid w_4) + \\log P(w_5\\mid w_4)+\\\\\n",
    " & \\log P(w_4\\mid w_5) )\n",
    "\\end{split}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's talk about the elephant in the room, $P(w_{t+j}\\mid w_{t};\\theta)$!\\\n",
    "We said before that it is the probability of having a context word given the centre word. However, how can we calculate such a probability? As it can be seen, we are dealing with a conditional probability or, more precisely, a probability distribution. Take another look at the result of the previous example. The last 5 lines show-for each word of 5 words - what is the probability of having such word given its context words ( lines one and five have just one expression since the only context of words 1 and 5 are respectively words 2 and 4). In order to calculate this probability, we can use a trick: Considering two different vector representations for each word based on its position, showing it with $v$ when it's a centre word and mentioning it with $u$ when it's a context word. Having such assumption, we can use the following formula:\n",
    "\n",
    "$$P(o\\mid c)=\\frac{e^{u_{o}^{T}v_{c}}}{\\sum_{w\\in V}e^{u_{w}^{T}v_{c}}}$$\n",
    "$u_{o}$: vector of context word\\\n",
    "$v_{c}$: vector of center word\\\n",
    "$u_{w}$: vector of a word not in a center position\\\n",
    "$P(o\\mid c)$: Probability of having Outside word $o$ having center word $c$\n",
    "\n",
    "What we see in this formula is a combination of two things:\n",
    "- Dot product of two vectors\n",
    "- Softmax function\n",
    "\n",
    "It means we can re-write our formula in the following way:\n",
    "\n",
    "$$P(o\\mid c) = Softmax(Probability(\\textbf{o}\\,\\,being\\,\\,context\\,\\,for\\,\\textbf{c})) = Softmax(dot\\,product(o.c))$$\n",
    "\n",
    "**Dot-product**\\\n",
    "By definition, a vector has two values: Magnitude and direction. \n",
    "\n",
    "By calculating the dot product of two vectors we are representing both magnitude and directions of both vectors:\n",
    "\n",
    "$$a.b = \\left | a \\right |\\times \\left | b \\right | \\times \\cos( \\theta)$$\n",
    "$\\left | a \\right |,\\left | b \\right |$: length(magnitude) of $a$ and $b$ vectors.\\\n",
    "$\\theta$ : Angle between directions of $a$ and $b$ vectors.\n",
    "\n",
    "\n",
    "Looking at the formula above, it is pretty easy to understand how the magnitude of vectors contribute to the outcome; the role of $\\cos(\\theta)$ might be less evident. We know that $\\cos(\\theta)$ should somehow reflect the angle between two vectors, but there are two questions here:\n",
    "\n",
    "- Why do we need to use angle in the first place? why using vector lengths is not enough?\n",
    "- How exactly $\\cos(\\theta)$ represents the angle? Let us say, why not use $sin(\\theta)$ instead?\n",
    "\n",
    "\n",
    "To answer the first question, let us first remind ourselves of the goal of a dot-product: combining two vectors to obtain their product. So we can expect that having vectors as ingredients, we need to consider all their characteristics which, as we mentioned before, are magnitude and direction. For example, looking at the following simple example, we can see that it is trivial to think that given that both vectors have the exact directions, it's possible to say that we can obtain the multiplication of these vectors by multiplying their lengths:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/simple_vectors.png\" width=\"250\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left | a \\right |\\times \\left | b \\right |  $$\\\n",
    "But the things won't remain always as simple as we have seen above. Look at the this example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/simple_cosine.png\" width=\"400\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three vectors $\\overrightarrow{a}, \\overrightarrow{b}$ and $\\overrightarrow{c}$ have the same length ($\\left | a \\right | = \\left | b \\right | = \\left | c \\right |$) but we in order to calculate $a.b$ or $a.c$ we can't just simply multiply their lengths because we can just do such a thing only if two vectors have the same direction. So, we need to convert one of the vectors somehow to have the same direction as the other vector. The way we can achieve this is to use the trigonometry $Cosine$ function. As you can see in the figure above, after converting  $\\overrightarrow{b}$ and  $\\overrightarrow{c}$ using $Cosine$ function we can see that $b.a$ will be greater than $c.a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The situation we face dealing with words will differ from the example above in 2-dimensional space. As we said before, even if we are going to use dense vectors instead of the sparse outcome of one-hot encoding, the vector space still would be too complex to be visually demonstrated.\n",
    "\n",
    " The figure below shows a 3-dimensional vector space with three vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/3d_cosine.png\" width=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Even without calculating dot products, we can almost certainly say that $a.b$ is greater than $a.c$ given that $\\phi$ the angle between $a$ and $b$ is close to $90^{\\circ}$ which brings $c.b$ close to zero. As a result, we can see that in our case of word vectors, we can expect that similar vectors have a higher dot product concerning the different vectors.\n",
    "\n",
    "Back to our formula we can see that we have two parts indicating a dot product: $u_{o}^{T}v_{c}$ and $u_{w}^{T}v_{c}$ .As you can see, we are using matrix transpose since we cannot directly multiply two columns vectors. Following is the general form of dot product:\n",
    "\n",
    "$$A^{T}B = \n",
    "\\begin{bmatrix}\n",
    "  A_1, A_2, \\dots, A_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  B_1 \\\\ B_2 \\\\ \\vdots \\\\ B_n\n",
    "\\end{bmatrix} = \n",
    "A_1 B_1 + A_2 B_2 + \\vdots + A_n B_n = A.B\n",
    "$$\n",
    "\n",
    "**Softmax Function**\\\n",
    "Before explaining the softmax function, let's take a quick look at the equation we have started with:\n",
    "\n",
    "\n",
    "$$\\frac{e^{\\color{blue}u_{\\color{blue}o}^\\color{blue}{T}\\color{blue}v_{\\color{blue}c}}}{\\sum_{w\\in V}e^{\\color{blue}u_{\\color{blue}w}^\\color{blue}{\\color{blue}T}\\color{blue}v_{\\color{blue}c}}}$$\n",
    "\n",
    "The blue parts are the pairwise dot-product of vectors, as we have discussed in the previous part. Now we should explore the rest of the equation.\n",
    " What we see in the above equation is implementation of dot-products in the following function:\n",
    "\n",
    "$$\\sigma (z)_i =\\frac{e^{z_{i}}}{\\sum_{j=1}^{K}e^{z_{j}}} \\,\\,\\,\\,\\,\\,\\,\\,\\,for\\,\\,i=1,\\dots,K$$\n",
    "\n",
    " What we can infer from this function is that it is calculating some proportion of the total sum, something like this:\n",
    "\n",
    "\n",
    "$$\\frac{Something}{Sum(Everything)}$$\n",
    "\n",
    " The first thing  comes to mind is to use it to understand how dot-product of a certain word and a center word is different from dot-product of that word with other words of the corpus. Something like this:\n",
    "\n",
    "$$\\frac{Probability(A\\,certain\\,word\\,being\\,context\\,of\\,a\\, specific\\,word)}{Sum(Probability(A\\,certain\\,word\\,being\\,context\\, of\\,any\\,other\\,word\\,in\\,the\\,corpus))}$$\n",
    "\n",
    " So maybe we can write a equation similar to this:\n",
    "\n",
    "$$\\frac{u_{o}^{T}v_{c}}{\\sum_{w\\in V}u_{w}^{T}v_{c}}$$\n",
    "\n",
    " The above equation gives us a vector with length of $V$ (our corpus) which each element is the following proportion:\n",
    "\n",
    "$$\\frac{dot\\ product(A\\,certain\\,word\\,being\\,context\\,of\\,a\\, specific\\,word)}{Sum(dot\\ product(A\\,certain\\,word\\,being\\,context\\, of\\,any\\,other\\,word\\,in\\,the\\,corpus))}$$\n",
    "\n",
    "Sounds great! There is just a tiny problem! Going back to the initial equation of likelihood, we should have had a probability part but What we get from the formula above is not a probability!\n",
    "There are two problems:\n",
    "\n",
    "- Sum of the items in the vector will not be equal to $1$.\n",
    "- Each item is not necessarily between $0$ and $1$\n",
    "\n",
    "\n",
    " The reason is quite simple! The outcome of a dot-product is a \"scaler\", not a probability $(0\\leqslant x\\leqslant 1)$. So we need to somehow \"transform\" these scalers to probabilities. This is the reason in the initial formula we used $e$ to transform the scaler result of the dot products to a number between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, Until here, we saw all the pieces of Cost function one by one. or the last time, let us summarize what we said until now and put these separate pieces together:\n",
    "\n",
    "**Cost function should be minimized $\\overset{so}{\\Rightarrow}$\n",
    "Individual probabilities should be maximized $\\overset{so}{\\Rightarrow}$\n",
    "Word vectors should represent the relationship between words**\n",
    "\n",
    "\n",
    "It means the only way we can minimize the cost function is to come up with a high probability for a certain word being a context for another specific word, which in turn, means we should somehow find the correct vectors for words, but the question is: How can we find such vectors? The answer - at least from the conceptual point of view - is easy: Initiate each word with a random vector, calculate the Cost Function, change the vectors in a way that give a lower cost, calculate the cost and repeat these steps until the cost can't go lower (you reach the global minimum for the cost). The following figure shows a simplistic representation of these steps:\n",
    "\n",
    "We start with a random set of vectors ($\\theta_{i}$) and use it to calculate the cost ($L(\\theta_{i})$). Using a method called **gradient descent** (more on this later) we change the $\\theta$ - little by little - until we get to the $\\dot{\\theta}$ which returns the minimum cost possible ($L(\\dot{\\theta})$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/w2v_loss.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using gradient descent, eventually, we can arrive at the point (word vectors) that minimize the cost, but how does gradient descent work? Simply put, it means taking small and directed(non-random) steps toward the minimum point from the starting point (which in our case was randomly generated). To take these steps, we need to have to things:\n",
    "\n",
    "- How big should be the step ($\\alpha$)\n",
    "- What should be the direction we move $\\theta$ to\n",
    "\n",
    "To understand these parameters, let's see the classic example of **gradient descent**: Imagine that you're a rock climber. You go to the bottom of a mountain, park your car and start climbing. After 6 hours,  you're somewhere near the top of a mountain, but since it's getting dark (and you're tired), you decide to go down, back to where you've parked your car. There's a thick fog in the mountain, and while you know your final destination is down (in our case, down means minimizing our cost function), you can't see the whole way until the destination point. So in this case, you decide to choose a direction that seems right to you, take some steps, evaluate your position and choose another direction. Eventually, you'll(hopefully) get to your car by repeating these actions. This is all __Gradient Descent (GD)__ does. Starting from a given initial point, it first calculates the partial depravities(A partial derivative shows you the direction of the point that makes the function minimum or maximum) then, using the step ($\\alpha$) you've defined beforehand, it updates the $\\theta$ and recalculates the cost. Repeating these steps, we'll finish with the best set of vectors for our corpus.\n",
    "\n",
    "Now that we are familiar with the internal mechanism Word2Vec uses to calculate the word vectors, let's take a quick look at more detail on the tool it uses to such vectors.\n",
    "\n",
    "As you can remember, we have said that Word2Vec \"assigns random vectors to words and keeps modifying \n",
    "them until they produce the minimum cost possible\". In order to do this procedure, Word2Vec uses \n",
    "Neural Networks. Neural Networks are one of the most used algorithms for a wide range of applications, from forecasting the stock prices to adding cat ears to your selfies. Here we won't go into detail of \n",
    "NN, since doing so, needs its own lesson(lessons?) and will take us away from our goal, but to \n",
    "have a general idea about NN, let's take a quick look at it. Before jumping directly to NNs, let's start with\n",
    "a simple example: the Snake game. In this game, you should guide the snake to the point of having three rules:\n",
    " + You can choose between Horizontal and vertical moves but not both at the same time\n",
    " + The snake should not hit himself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/snake.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine a slightly different game:\n",
    "\n",
    "- You can move Horizontally, Vertically, Diagonally.\n",
    "- You can jump, disappear and re-appear\n",
    "- You can mix the previous two moves\n",
    "- There are fixed and moving obstacles\n",
    "\n",
    "\n",
    "As you can see, the simple Snake game suddenly becomes much more complex as you should control more parameters\n",
    ", face harsher obstacles and achieve a more challenging goal.\n",
    "Now for the last time, let's consider a different scenario: \n",
    "\n",
    "- We have 1,000,000 parameters to control\n",
    "- You can combine all these parameters, and each combination has a different result\n",
    "- Good luck!\n",
    "\n",
    "Well, this time it seems we need something more than our fantastic human brains! That's where\n",
    "Neural Networks come to help us. A Neural Network takes hundreds or thousands \n",
    "(depending on the problem in hand) of solved cases and, trying different parameters, finds out the\n",
    "set of them that produce less error(or are closer to the correct cases.)\n",
    "This is what happens inside the word2vec algorithm; as we gave it our corpus, it considers the position\n",
    "of words as the correct neighbouring for the words. As a result, using neural networks returns word vectors that represent these neighbouring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of this notebook, we saw some of the differences our initial example had with the actual Word2Vec. We then deliberately left out a critical aspect as we were not familiar with the maths behind the algorithm. This aspect is the **Training method**. We saw that in order to calculate the multi-class loss, we should do the following calculation:\n",
    "\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{T}\\log L(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-m\\leqslant j\\leqslant m}\\log P(w_{t+j}\\mid w_{t};\\theta)\n",
    "$$\n",
    "$$j\\neq 0$$\n",
    "\n",
    "Which is calculated based on :\n",
    "$$P(o\\mid c)=\\frac{e^{u_{o}^{T}v_{c}}}{\\sum_{w\\in V}e^{u_{w}^{T}v_{c}}}$$ \n",
    "\n",
    "The problem is that calculation of its denominator is computationally expensive. To resolve it, Word2Vec introduces two approximations, replacing the multi-class loss with a set of binary logistic losses:\n",
    "- Negative sampling\n",
    "- Hierarchical Softmax\n",
    "\n",
    "**Negative Sampling**:\n",
    "We consider the words in our context as positive and all the words not a part of the context of our target word as negative samples. \n",
    "\n",
    "**hierarchical Softmax**:\n",
    "In hierarchical Softmax, we represent every word as a set of codes (Calculated by a tree-based method called Huffman Tree) to calculate the probability for all vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, in all examples, we have used a specific case: predicting context words having the centre word. This, called skip-gram, is one of the two internal algorithms Word2Vec can use. The other algorithm is called Continuous Bag Of Words (CBOW), which is when we try to predict the centre word using the context words. While having a similar goal, these algorithms return slightly different vectors for words. It also means that the conditional probability formula we saw above will be modified accordingly:\n",
    "\n",
    "$$P(c\\mid o)=\\frac{e^{h_{c}^{T}v_{o}}}{\\sum_{w\\in V}e^{h_{w}^{T}v_{o}}}$$ \n",
    "while $h_c$ is the sum of the words in the context:\n",
    "$$h_c = \\sum_{w\\in V}x_c$$\n",
    "**Skip-gram**: The input to the model is $w_i$, and the output is $w_{i-2},w_{i-1},w_{i+1},w_{i+2}$. So the task here is _\"predicting the context given a word\"_.(Works well with small amount of the training data, represents well even rare words or phrases)\n",
    "\n",
    "__CBOW__: The input to the model is $ùë§_{ùëñ‚àí2},ùë§_{ùëñ‚àí1},ùë§_{ùëñ+1},ùë§_{ùëñ+2}$, the preceding and following words of the target word. The output of the neural network will be $ùë§_ùëñ$. so the task is _\" predicting the word given its context\"_ (Several times faster to train than the skip-gram, slightly better accuracy for the frequent words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/cbow_skipgram.png\" height=\"300\" width=\"800\">\n",
    "\n",
    "_Note : As a hyper-parameter, you should define the Max window size but don't forget that the algorithm won't necessarily use all the words in the window but instead, it randomly choose some of the words based on the distance they have from the target word (in a way that closer words have a higher chance to be picked)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, we have learned the math behind the Word2Vec algorithm until here. In order to perform the described calculations, we need a Neural Network architecture which requires having a basic understanding of Neural Networks.\n",
    "(In case you do not feel confident about NNs, take a look at [this](https://www.youtube.com/watch?v=aircAruvnKk&pbjreload=10) or [this](https://www.youtube.com/watch?v=BR9h47Jtqyw&t=1342s) videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/NN_1.png\" height=\"300\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Distributed Representations of Words and Phrases and their Compositionality (Milkov et al)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* [Gensim documentation](https://radimrehurek.com/gensim/index.html)\n",
    "* [RaRe-Technologies Github Page](https://github.com/RaRe-Technologies/gensim/blob/3c3506d51a2caf6b890de3b1b32a8b85f7566ca5/docs/notebooks/doc2vec-IMDB.ipynb)\n",
    "* [Multi-Class Text Classification with Doc2Vec & Logistic Regression by Susan Li](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)\n",
    "* [Gensim Word2Vec Tutorial by Pierre Megret](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial)\n",
    "* [An Artificial Language Evaluation of Distributional Semantic Models](https://www.aclweb.org/anthology/K17-1015.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
