{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/header.png\" width=\"1200\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welcome!**\n",
    "\n",
    "In this workshop we first see an introduction to concepts of numerical representation of text and Word-Embedding and as we go on we'll learn about the Word2Vec and Doc2Vec algorithms and see how can we implement them in a ML task.\n",
    "\n",
    "Please note that the main purpose of this workshop is to make familiar a beginner ML user with the mentioned concepts instead of focusing on the most efficient - or pythonic way - to write your code. There are several ways the codes you'll see here can be written in a better/more intelligent way but as a result, they may seem more complex to a non-expert Python coder. That's why I intentionally decided to not use Classes, decorators and tools like these.\n",
    "\n",
    "To run practical examples I use the Simpsons [dataset from Kaggle](https://www.kaggle.com/pierremegret/dialogue-lines-of-the-simpsons/download)  which consists more that 150k lines and covers more than 600(old!) episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is written in Python3 and needs the following libraries:\n",
    "\n",
    "* **pandas** _(pip install pandas)_\n",
    "* **numpy** _(python -m pip install --user numpy)_\n",
    "* **gensim** _(pip install --upgrade gensim)_\n",
    "* **tqdm** _(pip install tqdm)_\n",
    "* **matplotlib** _(python -m pip install -U matplotlib)_\n",
    "* **sklearn** _(pip install -U scikit-learn)_\n",
    "* **SpaCy** _(python3 -m pip install spacy)_ --> python -m spacy download en\n",
    "* **testfixtures** _(pip install testfixtures)_\n",
    "\n",
    "To make sure which one you already have and which you should install, try to run the cell with the title **Loading libraries** and install the missing libraries accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is word-embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime between 50,000 years and 2 million years ago, humans started to talk and what we know today as \"languages\" is the result of this long process. \n",
    "\n",
    "The reason for it is rather simple: to be able to communicate about the world around them which directly impacts their lives. So, they create \"representations\" (which are not limited to words) to transfer an idea/concept/meaning to another human. While at the beginning these words/sounds/... were just independent symbols, by evolution of human language, words were also evolved not only as symbols but also as a way to link other concepts together. This process has happened during thousands of years, and during this period, human brain was unable to create a model of the reality, to link words(symbols) not just assign them to a single concept/idea but to understand their dynamics with the other symbols. So when I ask you ``give me a pair of scissors to cut this rope``, if you can't find scissors and instead see a knife you will probably bring it to me because in the model your brain has from the reality, scissors are used to cut, just as a knife. While at the first glance it may seem trivial, actually trying to replicate this phenomenon outside the human brain won't be so simple! before going in deep to see what options are out, let‚Äôs review how human brain deals with words:\n",
    "\n",
    "<img src=\"Images/human_process.png\" height=\"300\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better grasp the concept of *creating representations*, let's take a look at a simple example:\n",
    "\n",
    "Imagine we have a bunch of apples and for some reasons we want to find out the apples which are similar and dissimilar to each other:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/mele.png\" height=\"300\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How, in your opinion we can represent an apple?\n",
    "\n",
    "Let's say we decide to *represent* each one using these characteristic:\n",
    "* Color\n",
    "* Height\n",
    "* Perimeter\n",
    "\n",
    "So now it's possible to show them in space, this means we just created a mathematical representation for our apples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/coor.jpeg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now, imagine that we have to do the same thing as a computer algorithm, starting from concepts and finishing with a model which represents the real world (language model)...not so easy, right? ;)\n",
    "We're lucky that we shouldn't start from the first step, mapping concepts/ideas to symbols. Human beings have done it for us during thousands of years!\n",
    "Unfortunately having words is not enough for creating our model. To do so we need something called Word-embedding, one of the methods used for creating a numerical representation of textual data. Why we need such a representation you ask? well, to find out, just try to give the following paragraph to a classification algorithm, as it it, a text!\n",
    "\n",
    "``Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.``\n",
    "\n",
    "I think now you're agree that we need to have a numerical representation instead ;)\n",
    "Is Word2Vec is the only way to perform a word-embedding? of course not!\n",
    "\n",
    "Actually there are two main category of methods:\n",
    "\n",
    "* __Frequency Based Methods__\n",
    "    * Count Vectors\n",
    "    * TF-IDF\n",
    "    * Co-Occurance Vectors\n",
    "    * ...\n",
    "* __Predication Based Methods__\n",
    "    * Word2Vec\n",
    "    * GloVe\n",
    "    * Tyrion\n",
    "    * ...\n",
    "\n",
    "_Prediction Bases Methods are both more sophisticated and computationally expensive respect to Frequency Based Methods._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the expected outcome of a Word2Vec model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, a Vector!\n",
    "\n",
    "To be more precise, a dense numerical vector with a pre-defined length.\n",
    "\n",
    "<img src=\"Images/word2vec.png\" height=\"300\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply put, a Word2Vec is just a simple neural network which performs a binary classification and uses the learned classifier weights as embeddings of words. All the classifier does is to return the probability of ___c___ being a context word for a target word ___t___. that's it!\n",
    "\n",
    "A Word2Vec model, internally uses one of the following algorithms to create the embeddings:\n",
    "\n",
    "* Continuous Bag Of Words (CBOW)\n",
    "* Skip-Gram (SG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General mechanism of Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've said earlier, Word2Vec uses a Neural Network in order to generate a corespondent vector for each word in the corpus. To understand the details about such an algorithm you need to have a basic understanding of Neural Networks.\n",
    "(In case you don't feel confident about NNs, take a look at [this](https://www.youtube.com/watch?v=aircAruvnKk&pbjreload=10) or [this](https://www.youtube.com/watch?v=BR9h47Jtqyw&t=1342s) videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/NN_1.png\" height=\"300\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extremely simplified version of Word2Vec algorithm would be:\n",
    "* set the first word of corpus as target\n",
    "* based on the defined window size, set the neighbors as context of the target word\n",
    "* select random words from neighbors based on their vicinity to the target\n",
    "* for each neighbor and target word create a fully connected NN with one layer and no activation function. the task of NN is to decide if a given word of corpus could be a context word for the target or not\n",
    "* the weight matrix of the NN's hidden layer is the vector of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following example shows a corpus with size of V and window size of 2:\n",
    "<img src=\"Images/NN_2.png\" height=\"700\" width=\"700\">\n",
    "\n",
    "<img src=\"Images/diff.png\" height=\"700\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between CBOW and SG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CBOW__: The input to the model is $ùë§_{ùëñ‚àí2},ùë§_{ùëñ‚àí1},ùë§_{ùëñ+1},ùë§_{ùëñ+2}$, the preceding and following words of the target word. The output of the neural network will be $ùë§_ùëñ$. so the task is __\"predicting the word given its context\"__\n",
    "\n",
    "\n",
    "__Skip-gram__: The input to the model is $ùë§_ùëñ$, and the output is $ùë§_{ùëñ‚àí2},ùë§_{ùëñ‚àí1},ùë§_{ùëñ+1},ùë§_{ùëñ+2}$. So the task here is __\"predicting the context given a word\"__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/cbow_skipgram.png\" height=\"300\" width=\"800\">\n",
    "\n",
    "_Note : As a hyperparameter you should define the Max window size but don't forget that the algorithm won't necessarily use all the words in the window but instead it randomly choose some of the words based on the distance they have from the target word (in a way that closer words have a higher chance to be picked)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which one should I use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the author of Word2Vec:\n",
    "\n",
    "* __Skip-gram__: works well with small amount of the training data, represents well even rare words or phrases. \n",
    "* __CBOW__: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.\n",
    "\n",
    "So it depends on your data and computational power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows the general process we will follow in this workshop\n",
    "<img src=\"Images/process_2.png\" height=\"300\" width=\"1100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:14:05.372015Z",
     "start_time": "2019-03-14T16:14:05.325445Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import gensim.parsing as gm\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:15:03.583831Z",
     "start_time": "2019-03-14T16:15:03.560700Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "warnings.filterwarnings('ignore')\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{cwd}/Datasets/simpsons_dataset.csv',\n",
    "                sep=',').dropna().reset_index(drop=True)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like any other ML task, we should start our work by preprocessing the data. To start let's use gensim's text processing tools to define a function. Consider that this function is created for this specific dataset. For example, it doesn't clean html tags, URLs,... since we know that they won't be present in our data. As you can see I commented out the stem_text function since we later use SpaCy to Lemmatize the text. (More on Lemmatization :\n",
    "[Lemmatization Approaches with Examples in Python](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:16:00.278280Z",
     "start_time": "2019-03-14T16:16:00.269003Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_preproc(text: str):\n",
    "    \"\"\"Performs pre-processing steps on the given string.\n",
    "    \n",
    "    Pre-processing is done using methods from gensim module.\n",
    "    Methods are hard-coded in the function.\n",
    "    \n",
    "    Args:\n",
    "        text: A string\n",
    "    \n",
    "    Returns:\n",
    "        A list with tokenized and Pre-processed tokens of the given string.\n",
    "    \"\"\"\n",
    "\n",
    "    my_filter = [\n",
    "        lambda x: x.lower(),\n",
    "        gm.strip_punctuation,\n",
    "        gm.strip_multiple_whitespaces,\n",
    "        gm.strip_numeric,\n",
    "        gm.remove_stopwords,\n",
    "        gm.strip_short,\n",
    "        gm.strip_tags,\n",
    "        #gm.stem_text\n",
    "    ]\n",
    "    return preprocess_string(text, filters=my_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the column which contains the text\n",
    "text_column = 'spoken_words'\n",
    "data = df[text_column]\n",
    "\n",
    "# passing texts to the function we defined above\n",
    "training_set_raw = []\n",
    "for d in tqdm_notebook(data, desc='Pre-Processing: Cleaning'):\n",
    "    training_set_raw.append((text_preproc(d)))\n",
    "\n",
    "# initialize spacy 'en' model\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# creating sentences from tokens\n",
    "training_sentence = [' '.join(d) for d in training_set_raw]\n",
    "\n",
    "# lmmatizing sentences\n",
    "training_set_0 = []\n",
    "for ts in tqdm_notebook(training_sentence, desc='Pre-Processing: Lemmatizing'):\n",
    "    doc = nlp(ts)\n",
    "    lemm = [token.lemma_ for token in doc]\n",
    "    if len(lemm) is not 0:\n",
    "        training_set_0.append(lemm)\n",
    "        \n",
    "# removing duplicated data\n",
    "training_set = []\n",
    "for l in tqdm_notebook(training_set_0, desc='Pre-Processing: Deduplicating'):\n",
    "    if l not in training_set:\n",
    "        training_set.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the result of our pre-processing for the first 5 sentences\n",
    "for i,j in zip(data[:5], training_set[:5]):\n",
    "    print(f\"-{i}\\n-{' '.join(j)}\")\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case we want to embed words using their neighbours, the words are frequently appear (or doesn't appear) close to them. Imagine that you want to calculate the occurrence probability of a set of three words : \n",
    "$$P(w_{1},w_{2},w_{3})$$\n",
    "We can write it in this way:\n",
    "$$P(w_{1},w_{2},w_{3}) = P(w_{1})P(w_{2}\\mid w_{1})P(w_{3}\\mid w_{1} w_{2})$$\n",
    "\n",
    "The general form of this formula is named Chain Rule:\n",
    "\n",
    "$$P(w_{1},w_{2},w_{3},...,w_{n}) = P(w_{1})P(w_{2}\\mid w_{1})...P(w_{n}\\mid w_{1}...w_{n-1})$$\n",
    "\n",
    "$$or$$\n",
    "\n",
    "$$P(w_{1}w_{2}...w_{i} )=\\prod  P(w_{i}\\mid w_{1} w_{2}...w_{i-1})$$\n",
    "\n",
    "_if you're interested in this topic take a look at [this video](https://www.youtube.com/watch?v=dkUtavsPqNA\n",
    ")_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's calculate the bigrams for our sentences. To do so we use Phrases class from Gensim library. According to its documentation Phrases automatically detect common phrases ‚Äì multi-word expressions / word n-grams ‚Äì from a stream of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:28:03.068567Z",
     "start_time": "2019-03-14T16:28:03.057239Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_ngrams(data: list, log: bool = True, n=2):\n",
    "    \"\"\"Creates ngrams(2-3) from the given list of tokenized and cleaned data.\n",
    "    \n",
    "    Args:\n",
    "        data: a list of ([tokens], key) pairs.\n",
    "        log: If True, it prints the status.\n",
    "        \n",
    "    Returns : \n",
    "        A Gensim Trigram object.\n",
    "    \"\"\"\n",
    "    if log:\n",
    "        print(\"Learning lexicon from files...\")\n",
    "\n",
    "    if log:\n",
    "        print(\"Creating Bigrams...\")\n",
    "    ngrams = Phrases(data, min_count=30)\n",
    "    \n",
    "    if log:\n",
    "        print(\"Bigrams are Ready.\")\n",
    "    if n ==3:\n",
    "        if log:\n",
    "            print(\"Creating Trigrams...\")\n",
    "        ngrams = Phrases(ngrams[data], min_count=10, threshold=2)\n",
    "        if log:\n",
    "            print(\"Trigrams are ready to use.\")\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:28:51.815706Z",
     "start_time": "2019-03-14T16:28:09.784414Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ngram = create_ngrams(training_set, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:33:06.503472Z",
     "start_time": "2019-03-14T16:33:06.491084Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_or_build_model(ngram: Phrases,\n",
    "                       skipgram: int=0,\n",
    "                       size: int=300,\n",
    "                       window: int=2,\n",
    "                       min_count: int=20,\n",
    "                       alpha=0.03, \n",
    "                       negative=20,\n",
    "                       hs: int=1,\n",
    "                       ns_exponent = 0.05,\n",
    "                       iter=10,\n",
    "                       load: str=None,\n",
    "                       save: str=None,\n",
    "                       log: bool=True):\n",
    "    \n",
    "    \"\"\"Build or if a saved model exists, load the model.\n",
    "\n",
    "    Args: \n",
    "        ngram: ngram object created by create_ngrams() function.\n",
    "        skipgram: internal algorithm used to create the model. 0=CBOW, 1=skipgram. [default = 0]\n",
    "        size: Dimensionality of the word vectors.\n",
    "        window: Maximum distance between the current and predicted word within a sentence.\n",
    "        min_count: Ignores all words with total frequency lower than this.\n",
    "        alpha: The initial learning rate\n",
    "        hs: hierarchical softmax if 0 non-zero, negative sampling will be used.[default = 1]\n",
    "        load: file path to the pre-built model to be loaded. If None, model will be created [default = None]\n",
    "        save: file path to be used to save the created model. If None, model won't be saved locally [default = None]\n",
    "        log: whether log messages should be printed for the use [default = True]\n",
    "    \n",
    "    Returns:\n",
    "        A word2vec model\n",
    "    \"\"\"\n",
    "    if log:\n",
    "        print(\"Creating ngrams...\")\n",
    "    sentences = [ngram[pair] for pair in tqdm_notebook(training_set)]\n",
    "\n",
    "    if load is not None:\n",
    "        if log:\n",
    "            print(\"Loading model...\")\n",
    "        model = Word2Vec.load(f\"./{load}\")\n",
    "        if log:\n",
    "            print(\"Model has been loaded successfully.\")\n",
    "    else:\n",
    "        if log:\n",
    "            print(\"Building Word2Vec...\")\n",
    "        model = Word2Vec(\n",
    "            sentences,\n",
    "            size=size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            sg=skipgram,\n",
    "            hs=hs,\n",
    "            alpha=alpha, \n",
    "            negative=negative,\n",
    "            ns_exponent = ns_exponent,\n",
    "            iter=iter)\n",
    "        if log:\n",
    "            print(\"Model has been built.\")\n",
    "        if save is not None:\n",
    "            if log:\n",
    "                print(\"Saving the model...\")\n",
    "            model.save(f\"./{save}\")\n",
    "            if log:\n",
    "                print(\"Model has been saved successfully.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:34:33.218212Z",
     "start_time": "2019-03-14T16:33:15.330851Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating Word2Vec model\n",
    "model = get_or_build_model(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:40:46.707640Z",
     "start_time": "2019-03-14T16:40:46.687953Z"
    }
   },
   "outputs": [],
   "source": [
    "def n_most_similar(model: Word2Vec,\n",
    "                   text: str,\n",
    "                   n: int = 10):\n",
    "    \"\"\"Prints the n most similar words of the given string.\n",
    "\n",
    "    Args:\n",
    "        model : An object of Word2Vec class, created by get_or_build_model().\n",
    "        text : the string we want to get it similar words.\n",
    "        n : number of similar words we want to get [default = 10]        \n",
    "    \"\"\"\n",
    "\n",
    "    most_similar_to = model.wv.most_similar(positive=[text], topn=n)\n",
    "    for i, similar_item in enumerate(most_similar_to):\n",
    "        similar_key = similar_item[0]\n",
    "        score = str(round(float(similar_item[1]), 2))\n",
    "        print(f\"{i+1} - {similar_key} -- {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:41:03.051897Z",
     "start_time": "2019-03-14T16:41:02.929664Z"
    }
   },
   "outputs": [],
   "source": [
    "n_most_similar(model, text='bart', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match([\"nelson\", \"bart\", \"milhouse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match([\"bart\", \"lisa\", \"milhouse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is actually the Cosine of two vectors A and B in a Way that if they are orthogonal to each other(not similarity) the Cosine will be 0 and in case they are equal, the Cosine would be equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$similarity\\,score = cos(\\theta ) = \\frac{A \\cdot B}{\\left \\| A  \\right \\| \\left \\| B  \\right \\|} =\\frac{\\sum_{i=1}^{n}A_{i}B_{i}}{\\sqrt{\\sum_{i=1}^{n}A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n}B_{i}^{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T16:42:23.273634Z",
     "start_time": "2019-03-14T16:42:23.269688Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.n_similarity(['bart'], ['lisa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you think we can visualize the results of our word-embedding model? the short answer is, we can't! :D\n",
    "Simply because we are human beings and our brain can't comprehend an object which has more than 3 dimensions! _(reminder: in our example we've created 300-dimension vectors!)_\n",
    "\n",
    "So what is the solution? Fortunately back in 2008 a smart guy named _Laurens van der Maaten_ came up with a method which can reduce a high-dimension model to a 2-3 dimension space.\n",
    "This technique is called __t-SNE__ which stands for __t-Distributed Stochastic Neighbor Embedding__ . Explanation of t-SNE is way beyond the scope of this class but if you're interested in the topic, here is a nice video which explains how does it work:[link](https://www.youtube.com/watch?v=NEaUSP4YerM) \n",
    "\n",
    "_Note: Remember that getting a good result from t-SNE is not so easy! there are LOTS of parameters you can/should modify inside t-SNE function. [Here](https://distill.pub/2016/misread-tsne/) is a super-useful article with an interactive tool which helps you to play with parameters and discover their impact on the final results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T14:50:54.454057Z",
     "start_time": "2019-03-14T14:50:54.446059Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_tsne(model,\n",
    "              n_components: int=2,\n",
    "              perplexity:int =20,\n",
    "              learning_rate: int=10,\n",
    "              n_iter: int=1000,\n",
    "              metric: str='euclidean'):\n",
    "    \n",
    "    \"\"\"t-SNE plot for word-embedding model.\n",
    "    \n",
    "    Args:\n",
    "        n_components: Dimension of the embedded space.\n",
    "        perplexity: The perplexity is related to the number of nearest neighbors that\n",
    "            is used in other manifold learning algorithms. Larger datasets\n",
    "            usually require a larger perplexity. \n",
    "        learning_rate: The learning rate for t-SNE is usually in the range [10.0, 1000.0].\n",
    "            If the learning rate is too high, the data may look like a 'ball' with any\n",
    "            point approximately equidistant from its nearest neighbours.\n",
    "        n_iter: Maximum number of iterations for the optimization. Should be at least 250.\n",
    "        metric: The metric to use when calculating distance between instances in a \n",
    "            feature array.\n",
    "    \"\"\"\n",
    "\n",
    "    X = model[model.wv.vocab]\n",
    "    # diminesion reduction from 300 to 50 with PCA\n",
    "    X = PCA(n_components=50).fit_transform(X)\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=n_components,\n",
    "        perplexity=perplexity,\n",
    "        learning_rate=learning_rate,\n",
    "        n_iter=n_iter,\n",
    "        metric=metric)\n",
    "\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    plt.figure(figsize=(13, 8))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=2, alpha=0.4)\n",
    "    plt.title('t-SNE plot', fontsize=20)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T15:33:32.803659Z",
     "start_time": "2019-03-14T14:50:58.116710Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_tsne(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_n(model, word, n:int = 10):\n",
    "    \n",
    "    \"\"\"t-SNE plot for top n similar words for the given input\n",
    "    \n",
    "    Args:\n",
    "        model : An object of Word2Vec class, created by get_or_build_model().\n",
    "        word : the string we want to plot similar words for\n",
    "        n : number of similar words we want to get [default = 10]   \n",
    "    \"\"\"\n",
    "    \n",
    "    X = model[model.wv.vocab]\n",
    "    tsne = TSNE(n_components=2,\n",
    "                  perplexity=20,\n",
    "                  learning_rate=10,\n",
    "                  n_iter=1000,\n",
    "                  metric='euclidean')\n",
    "\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "    tsne_df = pd.DataFrame([X_tsne[:, 0], X_tsne[:, 1]]).T\n",
    "    tsne_df.columns = ['X', 'Y']\n",
    "\n",
    "    tsne_df['words'] = model.wv.vocab.keys()\n",
    "\n",
    "    top = [x[0] for x in model.wv.most_similar(positive=[word], topn=n)]\n",
    "\n",
    "\n",
    "    df_point = tsne_df[tsne_df.words == word].reset_index(drop=True)\n",
    "    df_1 = tsne_df[tsne_df.words.isin(top)].reset_index(drop=True)\n",
    "    plt.scatter(df_1.X, df_1.Y)\n",
    "    plt.scatter(df_point.X, df_point.Y, c='r')\n",
    "    for i in range(len(df_1)):\n",
    "        plt.text(df_1.X[i], df_1.Y[i], df_1.words[i])\n",
    "    plt.text(df_point.X[0], df_point.Y[0], df_point.words[0], horizontalalignment='center')\n",
    "    plt.title(f'Top {n} words for {word}', fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_n(model, 'homer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully now that you know the concept of word embeddings and Word2Vec algorithm, it's much more easier to talk about Doc2Vec.\n",
    "\n",
    "Doc2Vec is a generalization of a Word2Vec algorithm which not only takes into consideration the context of words, but also takes into account the context of the the document as a whole.\n",
    "\n",
    "In the doc2vec architecture, the two algorithm that are **‚Äúcontinuous bag of words‚Äù (CBOW)** and **‚Äúskip-gram‚Äù (SG)**; correspond to the **‚Äúdistributed memory‚Äù (DM)** and **‚Äúdistributed bag of words‚Äù (DBOW)**.\n",
    "\n",
    "The following figure shows how  Word2Vec integrates the Cocument ID into the context:\n",
    "\n",
    "\n",
    "<img src=\"Images/doc2vec.jpg\" height=\"300\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in Word2Vec example we used embeddings to create a visulization, in case of Doc2Vec we try to create a simple classifier which given a sentence can predict who's sentence is that!\n",
    "\n",
    "To do so we need to do a further step: **Balancing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the names\n",
    "df['raw_character_text'] = [x.lower().strip() for x in df['raw_character_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.raw_character_text.value_counts()[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see **Lisa** has the lowest lines : 10756 . One way to balance the data is down sample others to Lisa level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ho = df[df.raw_character_text == 'homer simpson'].sample(10756)\n",
    "data_ma = df[df.raw_character_text == 'marge simpson'].sample(10756)\n",
    "data_ba = df[df.raw_character_text == 'bart simpson'].sample(10756)\n",
    "data_li = df[df.raw_character_text == 'lisa simpson']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should individually divide data for each character to train and test and then combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ho_test = data_ho.sample(3226)\n",
    "data_ho_train = data_ho[~data_ho.index.isin(data_ho_test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ma_test = data_ma.sample(3226)\n",
    "data_ma_train = data_ma[~data_ma.index.isin(data_ma_test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ba_test = data_ba.sample(3226)\n",
    "data_ba_train = data_ba[~data_ba.index.isin(data_ba_test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_li_test = data_li.sample(3226)\n",
    "data_li_train = data_li[~data_li.index.isin(data_li_test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "data_train = pd.concat([data_ho_train, data_ma_train, data_ba_train, \n",
    "                      data_li_train]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "data_test = pd.concat([data_ho_test, data_ma_test, data_ba_test, \n",
    "                      data_li_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging each document with its label\n",
    "train_tagged = data_train.apply(lambda x: TaggedDocument(words=text_preproc(x['spoken_words']), tags=[x.raw_character_text]), axis=1)\n",
    "test_tagged = data_test.apply(lambda x: TaggedDocument(words=text_preproc(x['spoken_words']), tags=[x.raw_character_text]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example\n",
    "train_tagged.values[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, in order to create a Doc2Vec model we have two options. Let's start from DBOW model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializing the Doc2Vec model and creating the vocabulary\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=30, hs=0, min_count=6)\n",
    "model_dbow.build_vocab([x for x in tqdm_notebook(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the model in 30 epochs\n",
    "for epoch in tqdm_notebook(range(30), desc= 'Training model'):\n",
    "    model_dbow.train(utils.shuffle([x for x in train_tagged.values]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeled_vectors(model:Doc2Vec,\n",
    "                     tagged_docs:pd.core.series.Series):\n",
    "    \"\"\"Tagged vectors for the given model and documnets\n",
    "    \n",
    "    Args:\n",
    "        model: trained Doc2Vec model\n",
    "        tagged_docs: A pandas.Series which contains tagged documenst with their labels\n",
    "        \n",
    "    Returns:\n",
    "        labels and vectors for the giben documents\n",
    "        \n",
    "    \"\"\"\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = labeled_vectors(model_dbow, train_tagged)\n",
    "y_test, X_test = labeled_vectors(model_dbow, test_tagged)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_pred), 4)}')\n",
    "print(f'F1 score: {round(f1_score(y_test, y_pred, average=\"weighted\"),4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm = Doc2Vec(dm=1, window=3, vector_size=300, negative=30, min_count=6,alpha=0.1, min_alpha=0.1)\n",
    "model_dm.build_vocab([x for x in train_tagged.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the model in 30 epochs\n",
    "for epoch in tqdm_notebook(range(30), desc= 'Training model'):\n",
    "    model_dm.train(utils.shuffle([x for x in train_tagged.values]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dm.alpha -= 0.002\n",
    "    model_dm.min_alpha = model_dm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = labeled_vectors(model_dm, train_tagged)\n",
    "y_test, X_test = labeled_vectors(model_dm, test_tagged)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_pred), 4)}')\n",
    "print(f'F1 score: {round(f1_score(y_test, y_pred, average=\"weighted\"),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to creators of gensim library combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance. So le't do it first by deleting training data of **model_dbow** and **model_dm** to free up the momory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "# concatenating models\n",
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = labeled_vectors(new_model, train_tagged)\n",
    "y_test, X_test = labeled_vectors(new_model, test_tagged)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_pred), 4)}')\n",
    "print(f'F1 score: {round(f1_score(y_test, y_pred, average=\"weighted\"),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Distributed Representations of Words and Phrases and their Compositionality (Milkov et al)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* [Gensim documentation](https://radimrehurek.com/gensim/index.html)\n",
    "* [RaRe-Technologies Github Page](https://github.com/RaRe-Technologies/gensim/blob/3c3506d51a2caf6b890de3b1b32a8b85f7566ca5/docs/notebooks/doc2vec-IMDB.ipynb)\n",
    "* [Multi-Class Text Classification with Doc2Vec & Logistic Regression by Susan Li](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)\n",
    "* [Gensim Word2Vec Tutorial by Pierre Megret](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
